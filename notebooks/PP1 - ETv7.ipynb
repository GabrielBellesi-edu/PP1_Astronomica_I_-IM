{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "669d1a76-fd0c-4535-a67c-f7b6342f41aa",
   "metadata": {},
   "source": [
    "<h1>Juntando todo el Codigo - data_cleaning.py</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d95ff-5fca-46fc-844b-93c248fee371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculo/data_cleaning.py\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def convertir_mpd_a_csv(ruta_mpd=\"data/raw\", ruta_salida=\"data/interim/conversion\", filas_saltadas=29): \n",
    "    ruta_mpd = Path(ruta_mpd)      # Ruta a los .mpd\n",
    "    ruta_csv = Path(ruta_salida)   # Ruta donde guardar los .csv \n",
    "    ruta_csv.mkdir(parents=True, exist_ok=True) # Crea la carpeta si no existe\n",
    "\n",
    "        # Buscar todos los archivos .mpd\n",
    "    archivos_mpd = sorted(ruta_mpd.glob(\"mp*.riogrande.mpd\"))  \n",
    "    colspecs = [\n",
    "    (0, 11),     # Date (YYYY/MM/DD)\n",
    "    (12, 24),    # Time (hh:mm:ss.sss)\n",
    "    (25, 30),    # File ID (ej: 00RU0)\n",
    "    (31, 36),    # Rge\n",
    "    (37, 43),    # Ht\n",
    "    (44, 51),    # Vrad\n",
    "    (52, 58),    # delVr\n",
    "    (59, 65),    # Theta\n",
    "    (66, 73),    # Phi0\n",
    "    (75, 76),    # Ambig\n",
    "    (80, 85),    # Delphase\n",
    "    (89, 92),    # ant-pair\n",
    "    (99, 100),   # IREX\n",
    "    (102, 109),  # amax\n",
    "    (110, 114),  # Tau\n",
    "    (115, 120),  # vmet\n",
    "    (122, 127),  # snrdb\n",
    "    ]\n",
    "\n",
    "    for archivo in archivos_mpd:\n",
    "        # Notificacion de progreso. \n",
    "        print(f\"Convirtiendo: {archivo.name}\")\n",
    "        \n",
    "        # Leer datos ignorando las primeras filas. Avisa al pandas que viene sin header\n",
    "        df = pd.read_fwf(archivo, skiprows=filas_saltadas, header=None, colspecs=colspecs)\n",
    "\n",
    "        # Convertir la columna de fecha al formato DD/MM/AAAA\n",
    "        df[0] = pd.to_datetime(df[0], format=\"%Y/%m/%d\").dt.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "        # Nombre de archivo .csv en la nueva carpeta\n",
    "        nuevo_nombre = ruta_csv / (archivo.stem + \".csv\")\n",
    "\n",
    "        # Guardar como CSV con separador \";\" y decimal \",\" y sin agregar header\n",
    "        df.to_csv(nuevo_nombre, index=False, sep=\";\", decimal=\",\", header=False)\n",
    "\n",
    "    # Confirmacion de proceso terminado.\n",
    "    print(\"✅ Conversión completa.\")\n",
    "\n",
    "\n",
    "def filtro_ambig(ruta_entrada=\"data/interim/conversion\", ruta_salida=\"data/interim/transformacion\", \n",
    "                 columna_objetivo=9, valor_permitido=1, #De la columna 9 (ambig) Elimina todos los registros distintos a 1\n",
    "                 columnas_a_eliminar=[2, 9, 12]): #Elimina las columnas 2 (file) 9 (ambig) y 12 (IREX)\n",
    "    \n",
    "    ruta_entrada = Path(ruta_entrada)\n",
    "    ruta_salida = Path(ruta_salida)\n",
    "    ruta_salida.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    archivos_csv = sorted(ruta_entrada.glob(\"*.csv\"))\n",
    "\n",
    "    for archivo in archivos_csv:\n",
    "       \n",
    "        print(f\"Procesando: {archivo.name}\")\n",
    "        \n",
    "        # Leer el CSV sin encabezado\n",
    "        df = pd.read_csv(archivo, sep=\";\", header=None, decimal=\",\")\n",
    "\n",
    "        # Filtrar por ambigüedad\n",
    "        df_filtrado = df[df[columna_objetivo] == valor_permitido]\n",
    "\n",
    "        # Eliminar columnas especificadas\n",
    "        df_resultado = df_filtrado.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "        # Guardar resultado\n",
    "        nuevo_nombre = ruta_salida / archivo.name\n",
    "        df_resultado.to_csv(nuevo_nombre, index=False, sep=\";\", decimal=\",\", header=False)\n",
    "\n",
    "    print(\"✅ Filtro por ambig completado.\")\n",
    "\n",
    "\n",
    "# ⚠️ MODIFICACIÓN 3:\n",
    "# Se integró el tercer script como función dentro del módulo.\n",
    "# Se actualizó la ruta base para alinearla con el estándar de cookiecutter.\n",
    "# Se agregó mkdir con `parents=True` para evitar errores si la carpeta no existe.\n",
    "def filtro_rge(ruta_entrada=\"data/interim/transformacion\", ruta_salida=\"data/interim/filtrado\",\n",
    "               indice_rge=2, rge_min=100, rge_max=130):\n",
    "    \n",
    "    ruta_entrada = Path(ruta_entrada)\n",
    "    ruta_salida = Path(ruta_salida)\n",
    "    ruta_salida.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    archivos_csv = sorted(ruta_entrada.glob(\"*.csv\"))\n",
    "\n",
    "    for archivo in archivos_csv:\n",
    "        print(f\"Filtrando por Rge: {archivo.name}\")\n",
    "        \n",
    "        # Leer archivo filtrado previamente\n",
    "        df = pd.read_csv(archivo, sep=\";\", header=None, decimal=\",\")\n",
    "\n",
    "        # Aplicar filtro por Rge entre 100 y 130 inclusive\n",
    "        df_filtrado = df[(df[indice_rge] >= rge_min) & (df[indice_rge] <= rge_max)]\n",
    "\n",
    "        # Guardar archivo filtrado\n",
    "        nuevo_nombre = ruta_salida / archivo.name\n",
    "        df_filtrado.to_csv(nuevo_nombre, index=False, sep=\";\", decimal=\",\", header=False)\n",
    "\n",
    "    print(\"✅ Filtrado por Rge completado.\")\n",
    "\n",
    "\n",
    "def consolidar_csv(ruta_entrada=\"data/interim/filtrado\", ruta_salida=\"data/processed/2024_consolidado.csv\"):\n",
    "    ruta_entrada = Path(ruta_entrada)\n",
    "    archivos_csv = sorted(ruta_entrada.glob(\"*.csv\"))\n",
    "\n",
    "    # Lista de nombres de columnas, ya que los archivos CSV individuales no tienen encabezado\n",
    "    columnas = [\n",
    "        \"Date\", \"Time\", \"Rge\", \"Ht\", \"Vrad\", \"delVr\",\n",
    "        \"Theta\", \"Phi0\", \"Delphase\", \"ant-pair\",\n",
    "        \"amax\", \"Tau\", \"vmet\", \"snrdb\"\n",
    "    ]\n",
    "    \n",
    "    # Lista vacía para acumular los DataFrames de cada archivo\n",
    "    dataframes = []\n",
    "\n",
    "    # Iterar sobre cada archivo CSV individual\n",
    "    for archivo in archivos_csv:\n",
    "        \n",
    "        # Notificacion de progreso.\n",
    "        print(f\"Leyendo: {archivo.name}\")\n",
    "\n",
    "        # Leer el archivo sin encabezado, asigna los nombres de columnas definidas arriba y usando ; como separador y , como decimal\n",
    "        df = pd.read_csv(archivo, header=None, names=columnas, sep=\";\", decimal=\",\")\n",
    "        \n",
    "        # Agregar el DataFrame a la lista\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Unir todos los DataFrames\n",
    "    df_total = pd.concat(dataframes, ignore_index=True)\n",
    "    df_total.to_csv(ruta_salida, index=False, sep=\";\", decimal=\",\")\n",
    "\n",
    "    print(\"✅ Consolidación completada.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
